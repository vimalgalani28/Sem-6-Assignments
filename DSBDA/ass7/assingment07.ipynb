{"cells":[{"cell_type":"markdown","id":"a755d865","metadata":{},"source":["## Name: Omkar Gaikwad <br> Batch: L1 <br> Roll No: 31126"]},{"cell_type":"markdown","id":"676b2c93","metadata":{},"source":["### Problem Statement\n","\n","**Text Analysis**\n","\n","1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS tagging, stop words removal, Stemming and Lemmatization.\n","\n","2. Create representation of document by calculating Term frequency and inverse document frequency."]},{"cell_type":"code","execution_count":1,"id":"5eec3f75","metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import nltk\n","import pandas as pd\n","import numpy as np\n","\n","# Download the required components\n","# nltk.download()"]},{"cell_type":"code","execution_count":8,"id":"e8fea0d6","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Error loading puntk: Package 'puntk' not found in index\n"]},{"data":{"text/plain":["False"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('puntk')  \n"]},{"cell_type":"code","execution_count":4,"id":"ec200f6c","metadata":{},"outputs":[],"source":["# Load the text from the text files to string\n","text1 = \"\"\n","text2 = \"\"\n","\n","with open('testdoc.txt', 'r') as f:\n","    text1 = f.read()\n","\n","with open('testdoc2.txt', 'r') as f:\n","    text2 = f.read()"]},{"cell_type":"code","execution_count":5,"id":"0e88bc3c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Millions of people in India took part in an annual tree planting drive Sunday. More than 250 million\n"]}],"source":["# Print a part of the text\n","print(text1[:100])"]},{"cell_type":"code","execution_count":6,"id":"d1490e12","metadata":{},"outputs":[],"source":["# Make all the words as lower cased\n","text1 = text1.lower()"]},{"cell_type":"markdown","id":"174ac032","metadata":{},"source":["### Tokenization"]},{"cell_type":"markdown","id":"8e72464d","metadata":{},"source":["Tokenization is the first step when working with language tasks, it simplifies the input data by splitting it into sentences or words, as per the requirement"]},{"cell_type":"code","execution_count":9,"id":"cacf48e9","metadata":{},"outputs":[{"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/vimal/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-ec40824d20fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenize the texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentences1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwords1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentences2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/vimal/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"]}],"source":["# Tokenize the texts\n","sentences1 = nltk.sent_tokenize(text1)\n","words1 = nltk.word_tokenize(text1)\n","\n","sentences2 = nltk.sent_tokenize(text2)\n","words2 = nltk.word_tokenize(text2)\n","\n","# Print the tokenized output for the first text\n","print('The number of sentence tokens are: ', len(sentences1))\n","print('The first sentence is: ', sentences1[0])\n","print('\\nThe number of word tokens are: ', len(words1))\n","print('The first 5 words are: ', words1[:5])"]},{"cell_type":"markdown","id":"92dfd08c","metadata":{},"source":["### POS Tagging and Stop Words removal"]},{"cell_type":"code","execution_count":null,"id":"d8b83afa","metadata":{},"outputs":[{"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\omkar madhav gaikwad/nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\omkar madhav gaikwad/nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32m<ipython-input-13-d02c71c5a3f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The number of stopwords in english are: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nThe stop words in english are: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\omkar madhav gaikwad/nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"]}],"source":["# Remove stop words in english language like a, an, the\n","from nltk.corpus import stopwords\n","\n","stop_words = set(stopwords.words('english'))\n","print('The number of stopwords in english are: ', len(stop_words))\n","print('\\nThe stop words in english are: ', stop_words)"]},{"cell_type":"code","execution_count":null,"id":"aba50eb4","metadata":{},"outputs":[],"source":["words1 = [word for word in words1 if word not in stop_words]\n","\n","# Store the first sentence for later comparison\n","first_sent = sentences1[0]\n","\n","for i in range(len(sentences1)):\n","    words = nltk.word_tokenize(sentences1[i])\n","    words = [word for word in words if word not in stop_words]\n","    sentences1[i] = ' '.join(words)"]},{"cell_type":"code","execution_count":null,"id":"7f1b72b4","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["After removing stop words, the number of word tokens left are:  281\n","\n","First sentence before removal of stopwords was:  millions of people in india took part in an annual tree planting drive sunday.\n","First sentence after removal of stopwords is:  millions people india took part annual tree planting drive sunday .\n"]}],"source":["print('After removing stop words, the number of word tokens left are: ', len(words1))\n","print('\\nFirst sentence before removal of stopwords was: ', first_sent)\n","print('First sentence after removal of stopwords is: ', sentences1[0])"]},{"cell_type":"markdown","id":"33d3d1af","metadata":{},"source":["**Observation**\n","\n","- Earlier the length of word list was 434, but after removal of stopwords the length has reduced to 298. Thus, 136 words have been removed.\n","- Also, stopwords have been removed from the sentences."]},{"cell_type":"code","execution_count":null,"id":"9dff6b70","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["A few exmaples of tagged words from the first sentence are:  [('millions', 'NNS'), ('people', 'NNS'), ('india', 'VBP'), ('took', 'VBD'), ('part', 'NN'), ('annual', 'JJ'), ('tree', 'NN'), ('planting', 'VBG'), ('drive', 'JJ'), ('sunday', 'NN')]\n"]}],"source":["# POS tagging (part of speech)\n","\n","tagged_pairs = []\n","\n","for i in sentences1:\n","    \n","    #  Using Part of speech tagger\n","    tagged = nltk.pos_tag(words1)\n","    tagged_pairs.append(tagged)\n","\n","print('A few exmaples of tagged words from the first sentence are: ', tagged_pairs[0][:10])"]},{"cell_type":"markdown","id":"6d3230e2","metadata":{},"source":["### Stemming"]},{"cell_type":"code","execution_count":null,"id":"3998e961","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["First sentence before stemming was:  millions people india took part annual tree planting drive sunday .\n","First sentence after stemming is:  million peopl india took part annual tree plant drive sunday .\n","\n","\n","First 15 words before and after stemming are: \n","millions -> million\n","people -> peopl\n","india -> india\n","took -> took\n","part -> part\n","annual -> annual\n","tree -> tree\n","planting -> plant\n","drive -> drive\n","sunday -> sunday\n",". -> .\n","250 -> 250\n","million -> million\n","saplings -> sapl\n","planted -> plant\n"]}],"source":["# Performing stemming operation on the text\n","from nltk.stem import PorterStemmer\n","\n","stemmer = PorterStemmer()\n","\n","# Store the sentences and words in order to compare later\n","old_sent = [sent for sent in sentences1]\n","old_words = [word for word in words1]\n","\n","# Store the words list to compare later and also store the stemmed words\n","stemmed = {word : stemmer.stem(word) for word in words1} \n","\n","# Update the words1 list with stemmed words\n","words1 = [stemmer.stem(word) for word in words1]\n","\n","# Stem the words and update the sentence\n","for i in range(len(sentences1)):\n","    words = nltk.word_tokenize(sentences1[i])\n","    words = [stemmer.stem(word) for word in words]\n","    sentences1[i] = ' '.join(words)\n","    \n","print('First sentence before stemming was: ', old_sent[0])\n","print('First sentence after stemming is: ', sentences1[0])\n","\n","# Print the first 15 words before and after stemming\n","print('\\n\\nFirst 15 words before and after stemming are: ')\n","index = 0\n","for value in stemmed.items():\n","    print('{0} -> {1}'.format(value[0], value[1]))\n","    index += 1\n","    if index >= 15:\n","        break"]},{"cell_type":"markdown","id":"6cd198ab","metadata":{},"source":["### Lemmatization"]},{"cell_type":"code","execution_count":null,"id":"5acd49aa","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["First sentence before lemmatization was:  millions people india took part annual tree planting drive sunday .\n","First sentence after lemmatization is:  million people india took part annual tree planting drive sunday .\n","\n","\n","First 15 words before and after lemmatization are: \n","millions -> million\n","people -> people\n","india -> india\n","took -> took\n","part -> part\n","annual -> annual\n","tree -> tree\n","planting -> planting\n","drive -> drive\n","sunday -> sunday\n",". -> .\n","250 -> 250\n","million -> million\n","saplings -> sapling\n","planted -> planted\n"]}],"source":["from nltk.stem import WordNetLemmatizer\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","# Store the words list to compare later and also store the lemmatized words\n","lemmatized = {word : lemmatizer.lemmatize(word) for word in old_words} \n","\n","# Update the words1 list with stemmed words\n","words1 = [lemmatizer.lemmatize(word) for word in old_words]\n","\n","# Lemmatization\n","# Note that we will perform lemmatization on old sentences which are not stemmed\n","for i in range(len(old_sent)):\n","    words = nltk.word_tokenize(old_sent[i])\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","    sentences1[i] = ' '.join(words)  \n","    \n","print('First sentence before lemmatization was: ', old_sent[0])\n","print('First sentence after lemmatization is: ', sentences1[0])\n","\n","# Print the first 15 words before and after lemmatization\n","print('\\n\\nFirst 15 words before and after lemmatization are: ')\n","index = 0\n","for value in lemmatized.items():\n","    print('{0} -> {1}'.format(value[0], value[1]))\n","    index += 1\n","    if index >= 15:\n","        break"]},{"cell_type":"markdown","id":"423b5e23","metadata":{},"source":["### Term-Frequency and Inverse Document Frequency"]},{"cell_type":"code","execution_count":null,"id":"0fda6353","metadata":{},"outputs":[],"source":["# Get the unique words in the word list\n","word_set = set(words1)"]},{"cell_type":"code","execution_count":null,"id":"3e47a24a","metadata":{},"outputs":[],"source":["# First create an index for each word in our word list\n","index_dict = {}\n","i = 0\n","for word in word_set:\n","    index_dict[word] = i\n","    i += 1"]},{"cell_type":"code","execution_count":null,"id":"2d7f1e4c","metadata":{},"outputs":[],"source":["# Create a count dictionary to count the number of documents containing the word\n","def count_dict(sentences):\n","    word_count = {}\n","    for word in word_set:\n","        word_count[word] = 0\n","        for sent in sentences:\n","            if word in sent:\n","                word_count[word] += 1\n","    return word_count\n","\n","word_count = count_dict(sentences1)"]},{"cell_type":"code","execution_count":null,"id":"699ab04b","metadata":{},"outputs":[],"source":["# Function to calculate Term frequency (TF)\n","def term_freq(document, word):\n","    n = len(document)\n","    occurance = len([token for token in document if token == word])\n","    \n","    return occurance/n"]},{"cell_type":"code","execution_count":null,"id":"cf7dd246","metadata":{},"outputs":[],"source":["# Function to calculate IDF\n","def inverse_df(word):\n","    try:\n","        word_occurance = word_count[word]+1\n","    except:\n","        word_occurance = 1\n","    return np.log(len(sentences1)/word_occurance)"]},{"cell_type":"code","execution_count":null,"id":"a4de6df6","metadata":{},"outputs":[],"source":["# Combine Tf and idf functions\n","def tf_idf(sentence):\n","    vec = np.zeros((len(word_set),))\n","    words = nltk.word_tokenize(sentence)\n","    for word in words:\n","        tf = term_freq(sentence, word)\n","        idf = inverse_df(word)\n","\n","        value = tf*idf\n","        vec[index_dict[word]] = value\n","    \n","    return vec"]},{"cell_type":"code","execution_count":null,"id":"3b4bbea7","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["281\n","[0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.01050223 0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.00326351 0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.00079819]\n"]}],"source":["# Apply tf-idf encoding to sentences1 corpus\n","vectors = []\n","for sent in sentences1:\n","#     print(sent)\n","    vec = tf_idf(sent)\n","    vectors.append(vec)\n","\n","print(vectors[5])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
