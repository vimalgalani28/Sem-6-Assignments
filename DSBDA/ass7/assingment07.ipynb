{
"cells": [
{
"cell_type": "markdown",
"id": "a755d865",
"metadata": {},
"source": [
"## Name: Omkar Gaikwad <br> Batch: L1 <br> Roll No: 31126"
]
},
{
"cell_type": "markdown",
"id": "676b2c93",
"metadata": {},
"source": [
"### Problem Statement\n",
"\n",
"**Text Analysis**\n",
"\n",
"1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS tagging, stop words removal, Stemming and Lemmatization.\n",
"\n",
"2. Create representation of document by calculating Term frequency and inverse document frequency."
]
},
{
"cell_type": "code",
"execution_count": 1,
"id": "5eec3f75",
"metadata": {},
"outputs": [],
"source": [
"# Import necessary libraries\n",
"import nltk\n",
"import pandas as pd\n",
"import numpy as np\n",
"\n",
"# Download the required components\n",
"# nltk.download()"
]
},
{
"cell_type": "code",
"execution_count": null,
"id": "e8fea0d6",
"metadata": {},
"outputs": [
{
"name": "stdout",
"output_type": "stream",
"text": [
"showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
]
}
],
"source": [
"nltk.download()  \n"
]
},
{
"cell_type": "code",
"execution_count": 9,
"id": "ec200f6c",
"metadata": {},
"outputs": [],
"source": [
"# Load the text from the text files to string\n",
"text1 = \"\"\n",
"text2 = \"\"\n",
"\n",
"with open('testdoc.txt', 'r') as f:\n",
"    text1 = f.read()\n",
"\n",
"with open('testdoc2.txt', 'r') as f:\n",
"    text2 = f.read()"
]
},
{
"cell_type": "code",
"execution_count": 10,
"id": "0e88bc3c",
"metadata": {},
"outputs": [
{
"name": "stdout",
"output_type": "stream",
"text": [
"Millions of people in India took part in an annual tree planting drive Sunday. More than 250 million\n"
]
}
],
"source": [
"# Print a part of the text\n",
"print(text1[:100])"
]
},
{
"cell_type": "code",
"execution_count": 11,
"id": "d1490e12",
"metadata": {},
"outputs": [],
"source": [
"# Make all the words as lower cased\n",
"text1 = text1.lower()"
]
},
{
"cell_type": "markdown",
"id": "174ac032",
"metadata": {},
"source": [
"### Tokenization"
]
},
{
"cell_type": "markdown",
"id": "8e72464d",
"metadata": {},
"source": [
"Tokenization is the first step when working with language tasks, it simplifies the input data by splitting it into sentences or words, as per the requirement"
]
},
{
"cell_type": "code",
"execution_count": 12,
"id": "cacf48e9",
"metadata": {},
"outputs": [
{
"name": "stdout",
"output_type": "stream",
"text": [
"The number of sentence tokens are:  20\n",
"The first sentence is:  millions of people in india took part in an annual tree planting drive sunday.\n",
"\n",
"The number of word tokens are:  433\n",
"The first 5 words are:  ['millions', 'of', 'people', 'in', 'india']\n"
]
}
],
"source": [
"# Tokenize the texts\n",
"sentences1 = nltk.sent_tokenize(text1)\n",
"words1 = nltk.word_tokenize(text1)\n",
"\n",
"sentences2 = nltk.sent_tokenize(text2)\n",
"words2 = nltk.word_tokenize(text2)\n",
"\n",
"# Print the tokenized output for the first text\n",
"print('The number of sentence tokens are: ', len(sentences1))\n",
"print('The first sentence is: ', sentences1[0])\n",
"print('\\nThe number of word tokens are: ', len(words1))\n",
"print('The first 5 words are: ', words1[:5])"
]
},
{
"cell_type": "markdown",
"id": "92dfd08c",
"metadata": {},
"source": [
"### POS Tagging and Stop Words removal"
]
},
{
"cell_type": "code",
"execution_count": 13,
"id": "d8b83afa",
"metadata": {},
"outputs": [
{
"ename": "LookupError",
"evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\omkar madhav gaikwad/nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
"output_type": "error",
"traceback": [
"\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
"\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
"\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
"\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
"\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\omkar madhav gaikwad/nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
"\nDuring handling of the above exception, another exception occurred:\n",
"\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
"\u001b[1;32m<ipython-input-13-d02c71c5a3f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The number of stopwords in english are: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nThe stop words in english are: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
"\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
"\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
"\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
"\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
"\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\omkar madhav gaikwad/nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\omkar madhav gaikwad\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
]
}
],
"source": [
"# Remove stop words in english language like a, an, the\n",
"from nltk.corpus import stopwords\n",
"\n",
"stop_words = set(stopwords.words('english'))\n",
"print('The number of stopwords in english are: ', len(stop_words))\n",
"print('\\nThe stop words in english are: ', stop_words)"
]
},
{
"cell_type": "code",
"execution_count": 7,
"id": "aba50eb4",
"metadata": {},
"outputs": [],
"source": [
"words1 = [word for word in words1 if word not in stop_words]\n",
"\n",
"# Store the first sentence for later comparison\n",
"first_sent = sentences1[0]\n",
"\n",
"for i in range(len(sentences1)):\n",
"    words = nltk.word_tokenize(sentences1[i])\n",
"    words = [word for word in words if word not in stop_words]\n",
"    sentences1[i] = ' '.join(words)"
]
},
{
"cell_type": "code",
"execution_count": 8,
"id": "7f1b72b4",
"metadata": {},
"outputs": [
{
"name": "stdout",
"output_type": "stream",
"text": [
"After removing stop words, the number of word tokens left are:  281\n",
"\n",
"First sentence before removal of stopwords was:  millions of people in india took part in an annual tree planting drive sunday.\n",
"First sentence after removal of stopwords is:  millions people india took part annual tree planting drive sunday .\n"
]
}
],
"source": [
"print('After removing stop words, the number of word tokens left are: ', len(words1))\n",
"print('\\nFirst sentence before removal of stopwords was: ', first_sent)\n",
"print('First sentence after removal of stopwords is: ', sentences1[0])"
]
},
{
"cell_type": "markdown",
"id": "33d3d1af",
"metadata": {},
"source": [
"**Observation**\n",
"\n",
"- Earlier the length of word list was 434, but after removal of stopwords the length has reduced to 298. Thus, 136 words have been removed.\n",
"- Also, stopwords have been removed from the sentences."
]
},
{
"cell_type": "code",
"execution_count": 9,
"id": "9dff6b70",
"metadata": {},
"outputs": [
{
"name": "stdout",
"output_type": "stream",
"text": [
"A few exmaples of tagged words from the first sentence are:  [('millions', 'NNS'), ('people', 'NNS'), ('india', 'VBP'), ('took', 'VBD'), ('part', 'NN'), ('annual', 'JJ'), ('tree', 'NN'), ('planting', 'VBG'), ('drive', 'JJ'), ('sunday', 'NN')]\n"
]
}
],
"source": [
"# POS tagging (part of speech)\n",
"\n",
"tagged_pairs = []\n",
"\n",
"for i in sentences1:\n",
"    \n",
"    #  Using Part of speech tagger\n",
"    tagged = nltk.pos_tag(words1)\n",
"    tagged_pairs.append(tagged)\n",
"\n",
"print('A few exmaples of tagged words from the first sentence are: ', tagged_pairs[0][:10])"
]
},
{
"cell_type": "markdown",
"id": "6d3230e2",
"metadata": {},
"source": [
"### Stemming"
]
},
{
"cell_type": "code",
"execution_count": 10,
"id": "3998e961",
"metadata": {},
"outputs": [
{
"name": "stdout",
"output_type": "stream",
"text": [
"First sentence before stemming was:  millions people india took part annual tree planting drive sunday .\n",
"First sentence after stemming is:  million peopl india took part annual tree plant drive sunday .\n",
"\n",
"\n",
"First 15 words before and after stemming are: \n",
"millions -> million\n",
"people -> peopl\n",
"india -> india\n",
"took -> took\n",
"part -> part\n",
"annual -> annual\n",
"tree -> tree\n",
"planting -> plant\n",
"drive -> drive\n",
"sunday -> sunday\n",
". -> .\n",
"250 -> 250\n",
"million -> million\n",
"saplings -> sapl\n",
"planted -> plant\n"
]
}
],
"source": [
"# Performing stemming operation on the text\n",
"from nltk.stem import PorterStemmer\n",
"\n",
"stemmer = PorterStemmer()\n",
"\n",
"# Store the sentences and words in order to compare later\n",
"old_sent = [sent for sent in sentences1]\n",
"old_words = [word for word in words1]\n",
"\n",
"# Store the words list to compare later and also store the stemmed words\n",
"stemmed = {word : stemmer.stem(word) for word in words1} \n",
"\n",
"# Update the words1 list with stemmed words\n",
"words1 = [stemmer.stem(word) for word in words1]\n",
"\n",
"# Stem the words and update the sentence\n",
"for i in range(len(sentences1)):\n",
"    words = nltk.word_tokenize(sentences1[i])\n",
"    words = [stemmer.stem(word) for word in words]\n",
"    sentences1[i] = ' '.join(words)\n",
"    \n",
"print('First sentence before stemming was: ', old_sent[0])\n",
"print('First sentence after stemming is: ', sentences1[0])\n",
"\n",
"# Print the first 15 words before and after stemming\n",
"print('\\n\\nFirst 15 words before and after stemming are: ')\n",
"index = 0\n",
"for value in stemmed.items():\n",
"    print('{0} -> {1}'.format(value[0], value[1]))\n",
"    index += 1\n",
"    if index >= 15:\n",
"        break"
]
},
{
"cell_type": "markdown",
"id": "6cd198ab",
"metadata": {},
"source": [
"### Lemmatization"
]
},
{
"cell_type": "code",
"execution_count": 11,
"id": "5acd49aa",
"metadata": {},
"outputs": [
{
"name": "stdout",
"output_type": "stream",
"text": [
"First sentence before lemmatization was:  millions people india took part annual tree planting drive sunday .\n",
"First sentence after lemmatization is:  million people india took part annual tree planting drive sunday .\n",
"\n",
"\n",
"First 15 words before and after lemmatization are: \n",
"millions -> million\n",
"people -> people\n",
"india -> india\n",
"took -> took\n",
"part -> part\n",
"annual -> annual\n",
"tree -> tree\n",
"planting -> planting\n",
"drive -> drive\n",
"sunday -> sunday\n",
". -> .\n",
"250 -> 250\n",
"million -> million\n",
"saplings -> sapling\n",
"planted -> planted\n"
]
}
],
"source": [
"from nltk.stem import WordNetLemmatizer\n",
"\n",
"lemmatizer = WordNetLemmatizer()\n",
"\n",
"# Store the words list to compare later and also store the lemmatized words\n",
"lemmatized = {word : lemmatizer.lemmatize(word) for word in old_words} \n",
"\n",
"# Update the words1 list with stemmed words\n",
"words1 = [lemmatizer.lemmatize(word) for word in old_words]\n",
"\n",
"# Lemmatization\n",
"# Note that we will perform lemmatization on old sentences which are not stemmed\n",
"for i in range(len(old_sent)):\n",
"    words = nltk.word_tokenize(old_sent[i])\n",
"    words = [lemmatizer.lemmatize(word) for word in words]\n",
"    sentences1[i] = ' '.join(words)  \n",
"    \n",
"print('First sentence before lemmatization was: ', old_sent[0])\n",
"print('First sentence after lemmatization is: ', sentences1[0])\n",
"\n",
"# Print the first 15 words before and after lemmatization\n",
"print('\\n\\nFirst 15 words before and after lemmatization are: ')\n",
"index = 0\n",
"for value in lemmatized.items():\n",
"    print('{0} -> {1}'.format(value[0], value[1]))\n",
"    index += 1\n",
"    if index >= 15:\n",
"        break"
]
},
{
"cell_type": "markdown",
"id": "423b5e23",
"metadata": {},
"source": [
"### Term-Frequency and Inverse Document Frequency"
]
},
{
"cell_type": "code",
"execution_count": 12,
"id": "0fda6353",
"metadata": {},
"outputs": [],
"source": [
"# Get the unique words in the word list\n",
"word_set = set(words1)"
]
},
{
"cell_type": "code",
"execution_count": 13,
"id": "3e47a24a",
"metadata": {},
"outputs": [],
"source": [
"# First create an index for each word in our word list\n",
"index_dict = {}\n",
"i = 0\n",
"for word in word_set:\n",
"    index_dict[word] = i\n",
"    i += 1"
]
},
{
"cell_type": "code",
"execution_count": 14,
"id": "2d7f1e4c",
"metadata": {},
"outputs": [],
"source": [
"# Create a count dictionary to count the number of documents containing the word\n",
"def count_dict(sentences):\n",
"    word_count = {}\n",
"    for word in word_set:\n",
"        word_count[word] = 0\n",
"        for sent in sentences:\n",
"            if word in sent:\n",
"                word_count[word] += 1\n",
"    return word_count\n",
"\n",
"word_count = count_dict(sentences1)"
]
},
{
"cell_type": "code",
"execution_count": 15,
"id": "699ab04b",
"metadata": {},
"outputs": [],
"source": [
"# Function to calculate Term frequency (TF)\n",
"def term_freq(document, word):\n",
"    n = len(document)\n",
"    occurance = len([token for token in document if token == word])\n",
"    \n",
"    return occurance/n"
]
},
{
"cell_type": "code",
"execution_count": 16,
"id": "cf7dd246",
"metadata": {},
"outputs": [],
"source": [
"# Function to calculate IDF\n",
"def inverse_df(word):\n",
"    try:\n",
"        word_occurance = word_count[word]+1\n",
"    except:\n",
"        word_occurance = 1\n",
"    return np.log(len(sentences1)/word_occurance)"
]
},
{
"cell_type": "code",
"execution_count": 17,
"id": "a4de6df6",
"metadata": {},
"outputs": [],
"source": [
"# Combine Tf and idf functions\n",
"def tf_idf(sentence):\n",
"    vec = np.zeros((len(word_set),))\n",
"    words = nltk.word_tokenize(sentence)\n",
"    for word in words:\n",
"        tf = term_freq(sentence, word)\n",
"        idf = inverse_df(word)\n",
"\n",
"        value = tf*idf\n",
"        vec[index_dict[word]] = value\n",
"    \n",
"    return vec"
]
},
{
"cell_type": "code",
"execution_count": 23,
"id": "3b4bbea7",
"metadata": {},
"outputs": [
{
"name": "stdout",
"output_type": "stream",
"text": [
"281\n",
"[0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.01050223 0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.         0.\n",
" 0.         0.00326351 0.         0.         0.         0.\n",
" 0.         0.         0.         0.         0.00079819]\n"
]
}
],
"source": [
"# Apply tf-idf encoding to sentences1 corpus\n",
"vectors = []\n",
"for sent in sentences1:\n",
"#     print(sent)\n",
"    vec = tf_idf(sent)\n",
"    vectors.append(vec)\n",
"\n",
"print(vectors[5])"
]
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.8.8"
}
},
"nbformat": 4,
"nbformat_minor": 5
}
